\documentclass[11pt]{article}
\usepackage{header}
\def\title{HW 11}

\begin{document}
\maketitle
\fontsize{12}{15}\selectfont

\begin{center}
    Due: Saturday, 4/12, 4:00 PM \\
    Grace period until Saturday, 4/12, 6:00 PM \\
\end{center}

\section*{Sundry}
Before you start writing your final homework submission, state briefly how you 
worked on it.  Who else did you work with?  List names and email addresses. 
(In case of homework party, you can just describe the group.)

\begin{center}
    \textcolor{blue}{
        Zachary Brandt \\
        \nolinkurl{zbrandt@berkeley.edu}
    }
\end{center}

\vspace{15pt}

\Question{Combined Head Count}

\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n15.pdf}{Note 15}}
\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n19.pdf}{Note 19}}
Suppose you flip a fair coin twice.
\begin{Parts}

\Part What is the sample space $\Omega$ generated from these flips?

\Part Define a random variable $X$ to be the number of heads. What is the 
distribution of $X$?

\Part Define a random variable $Y$ to be $1$ if you get a heads followed by a 
tails and $0$ otherwise. What is the distribution of $Y$?

\Part Compute the conditional probabilities $\Pr[Y = i \mid X = j]$ for all $i$, 
$j$.

\Part Define a third random variable $Z = X + Y$. Use the conditional 
probabilities you computed in part (d) to find the distribution of $Z$.

\end{Parts}

\begin{solution}

\begin{Parts}
    
\Part The sample space $\Omega$ generated from two flips is $\{HH, HT, TH, TT \}$.

\Part The distribution of the random variable $X$ is the collection of values
$\{(a, \Pr[X=a]):a \in \mathcal{A} \}$, which is displayed below:
\begin{center}
    \begin{tabular}{|>{\centering\arraybackslash}m{6cm}|c|c|}
    \hline
    \textbf{outcomes $\omega$} & \textbf{value of X (\# heads)} & 
    \textbf{probability of occurring} \\
    \hline
    \textit{TT} & 0 & $\frac{1}{4}$ \\
    \hline
    \textit{HT, TH} & 1 & $\frac{2}{4}$ \\
    \hline
    \textit{HH} & 2 & $\frac{1}{4}$ \\
    \hline
    \end{tabular}
\end{center}

\Part The distribution of the random variable $Y$ is displayed below:
\begin{center}
    \begin{tabular}{|>{\centering\arraybackslash}m{6cm}|c|c|}
    \hline
    \textbf{outcomes $\omega$} & \textbf{value of Y (heads, tails)} & 
    \textbf{probability of occurring} \\
    \hline
    \textit{TT, TH, HH} & 0 & $\frac{3}{4}$ \\
    \hline
    \textit{HT} & 1 & $\frac{1}{4}$ \\
    \hline
    \end{tabular}
\end{center}

\Part The conditional probabilities are displayed below:
\begin{align*}
    \text{$Y$ can only be 1 when HT present} \quad \Pr[Y = 0 \mid X = 0] &= 1, \\
    \text{TH also present} \quad \Pr[Y = 0 \mid X = 1] &= 0.5, \\
    \text{$Y$ can only be 1 when HT present} \quad \Pr[Y = 0 \mid X = 2] &= 1, \\
    \text{$Y$ can only be 1 when HT present} \quad \Pr[Y = 1 \mid X = 0] &= 0, \\
    \text{TH also present} \quad \Pr[Y = 1 \mid X = 1] &= 0.5, \\
    \text{$Y$ can only be 1 when HT present} \quad \Pr[Y = 1 \mid X = 2] &= 0
\end{align*}

\Part The distribution of the random variable $Z$ is displayed below:

\begin{center}
    \begin{tabular}{|>{\centering\arraybackslash}m{6cm}|c|c|}
    \hline
    \textbf{outcomes $\omega$} & \textbf{value of Z (X+Y)} & 
    \textbf{probability of occurring} \\
    \hline
    \textit{TT} & 0 & $\frac{1}{4}$ \\
    \hline
    \textit{TH} & 1 & $\frac{1}{4}$ \\
    \hline
    \textit{HH, HT} & 2 & $\frac{2}{4}$ \\
    \hline
    \end{tabular}
\end{center}

\end{Parts}

\end{solution}

\Question{Testing Model Planes}

\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n15.pdf}{Note 15}}
Amin is testing model airplanes. He starts with $n$ model planes which each 
independently have probability $p$ of flying successfully each time they are 
flown, where $0<p<1$. Each day, he flies every single plane and keeps the ones 
that fly successfully (i.e. don't crash), throwing away all other models. He 
repeats this process for many days, where each "day" consists of Amin flying all 
remaining model planes and throwing away any that crash. Let $X_i$ be the random 
variable representing how many model planes remain after $i$ days. Note that 
$X_0 = n$. Justify your answers for each part.
\begin{Parts}
    \Part What is the distribution of $X_1$? That is, what is $\Pr[X_1=k]$? 
    \Part What is the distribution of $X_2$? That is, what is $\Pr[X_2=k]$? 
    Recognize the distribution of $X_2$ as one of the famous ones and provide 
    its name and parameters.
    \Part Repeat the previous part for $X_t$ for arbitrary $t \geq 1$.
    \Part What is the probability that at least one model plane still remains 
    (has not crashed yet) after $t$ days? Do not have any summations in your 
    answer.
    \Part Considering only the first day of flights, is the event $A_1$ that the 
    first and second model planes crash independent from the event $B_1$ that 
    the second and third model planes crash? Recall that two events $A$ and $B$ 
    are independent if $\Pr[A\cap B] = \Pr[A]\Pr[B]$. Prove your answer using 
    this definition.
    \Part Considering only the first day of flights, let $A_2$ be the event that 
    the first model plane crashes 
    \emph{and} exactly two model planes crash in total. Let $B_2$ be the event 
    that 
    the second plane crashes on the first day. What must $n$ be equal to 
    in terms of $p$ such that $A_2$ is independent from $B_2$? Prove your answer 
    using the definition 
    of independence stated in the previous part.
    \Part Are the random variables $X_i$ and $X_j$, where $i<j$, independent? 
    Recall that two random variables $X$ and $Y$ are independent if $\Pr[X=k_1 
    \cap Y=k_2] = \Pr[X=k_1]\Pr[Y=k_2]$ for all $k_1$ and $k_2$. Prove your answer 
    using this definition.
\end{Parts}

\begin{solution}
    
\begin{Parts}
    
\Part The probability that the number of remaining planes on day 1 is some $k$,
is $\Pr[X_1 = k] = \binom{n}{k} p^k (1-p)^{n-k}$.

\Part There need to be at least $k$ planes left over from day 1 for there to be
$k$ planes that remain after day 2. So, from the Total Probability Law, the 
probability that there are 2 planes remaining on day 2 is the probability that 
2 remain and $k$ remained from day 1, or $k+1$ remained, and so until all $n$
planes remained from day 1. 
\[
    \begin{split}
        \Pr[X_2 = k] &= \Pr[X_2 = k, X_1 = k] + \Pr[X_2 = k, X_1 = k + 1]
        + \dots + \Pr[X_2 = k, X_1 = n] \\
        &= \sum_{i=k}^{n} \Pr[X_2 = k \mid X_1 = i] \Pr[X_1 = i] \\
        &= \sum_{i=k}^{n} \Pr[X_2 = k \mid X_1 = i] \cdot \binom{n}{i} p^i (1-p)^{n-i} \\
        &= \sum_{i=k}^{n} \binom{i}{k} p^k (1-p)^{i-k} \cdot \binom{n}{i} p^i (1-p)^{n-i} \\
        &= p^k (1-p)^{n-k} \sum_{i=k}^{n} \binom{i}{k} \binom{n}{i} p^i \\
        &= p^k (1-p)^{n-k} \sum_{i=k}^{n} \binom{n}{k} \binom{n - k}{i - k} p^i \\
        \text{($j=i-k$)} \quad &= \binom{n}{k} p^k (1-p)^{n-k} \sum_{j=0}^{n-k} \binom{n - k}{j} p^{j+k} \\
        &= \binom{n}{k} p^{2k} (1-p)^{n-k} \sum_{j=0}^{n-k} \binom{n - k}{j} p^{j} \cdot 1^{n-k-j} \\
        &= \binom{n}{k} p^{2k} (1-p)^{n-k} (1+p)^{n-k} \\
        &= \binom{n}{k} p^{2k} \left( (1-p)(1+p) \right)^{n-k} \\
        &= \binom{n}{k} p^{2k} (1-p^2)^{n-k} \\
        X_2 &\sim \mathrm{Binomial}(n, p^2)
    \end{split}
\]

The above is a binomial distribution on parameters $n$ and $p^2$, which makes sense
considering that every plane flight outcome is independent of another and each
plane has to fly successfully twice, i.e. $p$ then $p$, to remain. $1-p^2$
represents all those planes who did not make it twice. 

\Part Using this aformentioned logic, for an arbitrary $t \geq 1$, the
distribution of $X_t$ is then $X_t \sim \mathrm{Binomial}(n, p^t)$.

\Part The set of outcomes where at least one model plane still remains is exactly
the complement of the set of outcomes where no model planes remain after $t$ days.
Therefore, the probability is $1-(1-p^t)^n$. 

\Part If the planes are numbered like they are here, the probability that the
first two crash is $(1-p)^2$, and the probability that the second two crash is
also $(1-p)^2$. For the joint event $A \cap B$, the first three planes crash,
since the second plane doesn't crash twice, which happens with probability
$(1-p)^3$. However, $\Pr[A] \cdot \Pr[B] = (1-p)^2 \cdot (1-p)^2 = (1-p)^4
\neq (1-p)^3$, and therefore the events are not independent. 

\Part For events $A_2$ and $B_2$ to be independent, $n$ nust be equal to $p-1$.
This is because the probability $\Pr[A_2]$ is equal to $\binom{n}{1}(1-p)^2$, 
since the first plane must crash, and then there are $n$ other options for the
other plane. The probability $\Pr[B_2]$ is equal to $(1-p)$ since just the 
second plane has to crash. If $n=p-1$, then
\[
    \begin{split}
        \binom{p-1}{1}\cdot(1-p)^2 = (1-p)^3 &= \Pr[A_2 \cap B_2] \\
        (1-p)\cdot(1-p)^2 = (1-p)^3 &= \Pr[A_2] \cdot Pr[B_2].
    \end{split}
\]

\Part False, consider the case where there are more planes on a following day 
than there were on a previous day, which is impossible considering the problem 
setup. E.g. $\Pr[X_1 = 1 \mid X_2 = 2] = 0$ because there cannot be more planes
than there were on the initial day, however $\Pr[X_1 = 1]$ and $\Pr[X_2 = 2]$ 
are both not equal to zero. Therefore, $\Pr[X_1 = 1 \mid X_2 = 2] \neq \Pr[X_1 = 1]
\cdot \Pr[X_2 = 2]$, and the two events are not independent. 

\end{Parts}

\end{solution}

\Question{Fishy Computations}
\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n19.pdf}{Note 19}}
Assume for each part that the random variable can be modelled by a Poisson 
distribution.

\begin{Parts}

\Part Suppose that on average, a fisherman catches $20$ salmon per week. What 
is the probability that he will catch exactly $7$ salmon this week?

\Part Suppose that on average, you go to Fisherman's Wharf twice a year. What 
is the probability that you will go at most once in 2024?

\Part Suppose that in March, on average, there are $5.7$ boats that sail in 
Laguna Beach per day. What is the probability there will be \textit{at least} 
$3$ boats sailing throughout the \textit{next two days} in Laguna?

\Part Denote $X \sim \text{Pois}(\lambda)$. Prove that \[ \E[Xf(X)] = \lambda 
\E[f(X+1)] \] for any function $f$. 

\end{Parts}

\begin{solution}
    
\begin{Parts}
    
\Part The probability that a fisher catches exactly 7 salmon this week is $\Pr[X=7]
= \frac{20^7 \cdot e^{-20}}{7!}$.

\Part The probability that you will go to the Fisherman's Whart at most once in
2024 is $\Pr[X \leq 1] = \Pr[X=0] + \Pr[X=1] = \frac{2^0 \cdot e^{-2}}{0!} + 
\frac{2^1 \cdot e^{-2}}{1!} = \frac{3}{e^2}$.

\Part The combined random variable for the number of boats sailing throughout 
the next two days is $X_1 + X_2 \sim \text{Pois}(\lambda + \lambda)$. Then, the
probability that there are at least 3 boats observed is the complement of the 
probability that there are less than 3 boats observed, i.e., 
\[
    \begin{split}
        \Pr[X_1 + X_2 \geq 3] &= 1 - \Pr[X_1 + X_2 < 3] \\
        &= 1 - \Pr[X_1 + X_2 = 0] - \Pr[X_1 + X_2 = 1] - \Pr[X_1 + X_2 = 2] \\
        &= 1 - \frac{(11.4)^0}{0!}e^{11.4} - \frac{(11.4)^1}{1!}e^{11.4} - \frac{(11.4)^2}{2!}e^{11.4} \\
    \end{split}
\]

\Part The proof is as follows:
\[
    \begin{split}
        E[Xf(X)] &= \sum_x x f(x) \Pr[X=x] \\
        &= \sum_x x f(x) \Pr[X=x] \\
        &= \sum_x x f(x) \frac{\lambda^x}{x!} e^{-\lambda} \\
        &= \sum_x f(x) \frac{\lambda^x}{(x-1)!} e^{-\lambda} \\
        &= \sum_{i=0}^{\infty} f(i+1) \frac{\lambda^{i+1}}{i!} e^{-\lambda} \\
        &= \lambda \sum_{i=0}^{\infty} f(i+1) \frac{\lambda^{i}}{i!} e^{-\lambda} \\
        &= \lambda \sum_x f(x+1) \frac{\lambda^{x}}{x!} e^{-\lambda} \\
        &= \lambda \sum_x f(x+1) \Pr[X=x] \\
        &= \lambda E[f(X+1)] \\
    \end{split}
\]

\end{Parts}

\end{solution}

\Question{Such High Expectations}
\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n19.pdf}{Note 19}}
Suppose $X$ and $Y$ are independently drawn from a Geometric distribution with 
parameter $p$. For each of the below subparts, your answer must be simplified 
(i.e. NOT left in terms of a summation).

\begin{Parts}

\Part Compute $\Ex{\min\left(X,Y\right)}$.

\Part Compute $\Ex{\max\left(X,Y\right)}$.

\Part Compute $\Pr[X + Y \ge t]$    

\end{Parts}

\begin{solution}
    
\begin{Parts}
    
\Part I will define a new random variable $Z$ as $Z = \min(X,Y)$. Then, the 
computation is as follows:
\[
    \begin{split}
        \Ex{Z} &= (0 \times \Pr[Z=0]) + (1 \times \Pr[Z=1]) + (2 \times \Pr[Z=2]) + \dots \\
        &= \Pr[Z=1] + (\Pr[Z=2] + \Pr[Z=2]) + (\Pr[Z=3] +  \Pr[Z=3] + \Pr[Z=3]) \dots \\
        &= (\Pr[Z=1] + \Pr[Z=2] + \Pr[Z=3] + \dots) + (\Pr[Z=2] + \Pr[Z=3] + \dots) + \dots \\
        &= \Pr[Z \geq 1] + \Pr[Z \geq 2] + \Pr[Z \geq 3] + \dots \\
        &= \sum_{i=1}^{\infty} \Pr[Z \geq i] \\
        \text{$X$ \& $Y$ need to be >} \quad &= \sum_{i=1}^{\infty} \Pr[X \geq i] \Pr[Y \geq i] \\
        &= \sum_{i=1}^{\infty} (1-p)^{i-1} \cdot (1-p)^{i-1} \\
        &= \sum_{i=1}^{\infty} (1-p)^{2i-2} \\
        &= (1-p)^{-2} \sum_{i=1}^{\infty} (1-p)^{2i} \\
        &= \frac{1}{1-(1-p)^2}
    \end{split}
\]

\Part From before we can use the fact that $\Ex{Z} = \sum_{i=1}^{\infty} 
\Pr[Z \geq i]$ to see that 
\[
    \begin{split}
        \Ex{\max(X, Y)} &= \sum_{i=1}^{\infty} \Pr[\max(X, Y) \geq i] \\
        &= \sum_{i=1}^{\infty} 1 - \Pr[\max(X, Y) < i] \\
        &= \sum_{i=1}^{\infty} 1 - \Pr[X < i] \cdot \Pr[Y < i] \\
        &= \sum_{i=1}^{\infty} 1 - (1 - \Pr[X \geq i]) \cdot (1 - \Pr[Y \geq i]) \\
        &= \sum_{i=1}^{\infty} 1 - (1 - (1-p)^{i-1}) \cdot (1 - (1-p)^{i-1}) \\
        &= \sum_{i=1}^{\infty} 2(1-p)^{i-1} - (1-p)^{2i - 2}
    \end{split}
\]

\Part For any value $x$ of $X$, the value of $y$ of $Y$ must then satisfy the
inequality $y \geq t - x$. Therefore, the computation is
\[
    \begin{split}
        \Ex{X + Y \geq t} &= \sum_t \Pr[X + Y \geq t] \\
        &= \sum_{i=1}^{\infty} \Pr[X =i] \Pr[Y \geq t - i] \\
        &= \sum_{i=1}^{\infty} \Pr[X =i] \Pr[Y \geq t - i] \\
        &= \sum_{i=1}^{t-1} \Pr[X =i] \Pr[Y \geq t - i] + \sum_{i=t}^{\infty} \Pr[X =i] \Pr[Y \geq t - i] \\
        \text{$Y$ has no bearing once $X\geq t$} \quad &= \sum_{i=1}^{t-1} \Pr[X =i] \Pr[Y \geq t - i] + \sum_{i=t}^{\infty} \Pr[X =i] \\
        &= \sum_{i=1}^{t-1} (1-p)^{i-1}p \cdot (1-p)^{t-i-1} + \Pr[X \geq t] \\\
        &= (1-p)^{t-2}p \sum_{i=1}^{t-1} 1 + \Pr[X \geq t] \\\
        &= (1-p)^{t-2}p (t-1)+ (1-p)^{t-1} \\\
    \end{split}
\]

\end{Parts}

\end{solution}

\Question{Swaps and Cycles}
\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n15.pdf}{Note 15}}
A permutation of $n$ objects is a bijection from $(1,\ldots,n)$ to itself. For 
example, the permutation $\pi=(2,1,4,3)$ of $4$ objects is the mapping $\pi(1) 
= 2$, $\pi(2) = 1$, $\pi(3) = 4$, and $\pi(4) = 3$. We'll say that a permutation 
$\pi = (\pi(1),\ldots,\pi(n))$ contains a \emph{swap} if there exist 
$i,j\in\{1,\ldots,n\}$ so that $\pi(i) = j$ and $\pi(j) = i$, where $i \neq j$. 
The example above contains two swaps: $(1,2)$ and $(3,4)$.

\begin{Parts}
	\Part In terms of $n$, what is the expected number of swaps in a random 
    permutation?
	
	\Part In the same spirit as above, we'll say that $\pi$ contains a 
    \emph{$k$-cycle} if there exist $i_1,\ldots,i_k \in \{1,\ldots,n\}$ with 
    $\pi(i_1) = i_2,\pi(i_2) = i_3,\ldots,\pi(i_k) = i_1$. Compute the 
    expectation of the number of $k$-cycles. 	
\end{Parts}

\begin{solution}
    
\begin{Parts}
    
\Part For a set of $n$ objects, there are $n!$ permutations, which make sup the
sample space. When two elements are swapped, there are $n-2$ remaining elements
which can be in any order, i.e., $(n-2)!$ permutations with any two elements
swapped. There are $n$ choose 2 potential pairs out of the $n$ objects. Since
each pair has the same probability of being swapped, $\frac{(n-2)!}{n!}$, the
expected value is $\binom{n}{2} \times \frac{(n-2)!}{n!} = \frac{n!}{(n-2)! \cdot 2!}
\times \frac{(n-2)!}{n!} = \frac{1}{2!} = \frac{1}{2}$. 

\Part For any subset of $k$ elements, there are $k!$ possible permutations, but
each permutation has $k$ equivalents, i.e. (1, 2, 3) is equivalent to (2, 3, 1),
so there are $k! \div k$ cycles for a $k$, and there are $n$ choose $k$ different
$k$-lenght subsets for a total of $\binom{n}{k} \times \frac{k!}{k}$ cycles. Then, 
for any permutation of $n$ objects, if there is a permutation with $k$ elements, 
the remaining $n-k$ elements can be ordered however, so the probability of getting
a cycle is $(n-k)! \div n!$. The expected value is then the number of such cycles
times the probability, i.e., 
\[
    \binom{n}{k} \times \frac{k!}{k} \times \frac{(n-k)!}{n!} = \frac{n!}{(n-k)!k!}
    \times \frac{k!}{k} \times \frac{(n-k)!}{n!} = \frac{1}{k}. 
\]  


\end{Parts}

\end{solution}

\end{document}
