\documentclass[11pt]{article}
\usepackage{header}
\def\title{HW 11}

\begin{document}
\maketitle
\fontsize{12}{15}\selectfont

\begin{center}
    Due: Saturday, 4/19, 4:00 PM \\
    Grace period until Saturday, 4/19, 6:00 PM \\
\end{center}

\section*{Sundry}
Before you start writing your final homework submission, state briefly how you 
worked on it. Who else did you work with?  List names and email addresses. (In 
case of homework party, you can just describe the group.)

\begin{center}
    \textcolor{blue}{
        Zachary Brandt \\
        \nolinkurl{zbrandt@berkeley.edu}
    }
\end{center}

\vspace{15pt}

\Question{Coupon Collector Variance}

\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n19.pdf}{Note 19}}
It's that time of the year again---Safeway is offering its Monopoly Card 
promotion. Each time you visit Safeway, you are given one of $n$ different 
Monopoly Cards with equal probability. You need to collect them all to redeem 
the grand prize.

Let $X$ be the number of visits you have to make before you can redeem the grand 
prize. Show that $\var(X) = n^2\left(\sum_{i=1}^n i^{-2}\right) - \E[X]$.

\begin{solution}

We know from the notes that the expected value of $X$ is $\E[X] = n \cdot 
\sum_{i=1}^{n} \frac{1}{n}$. The variance of $X$, where $X$ is equal to $X_1 + 
X_2 + \cdots + X_n$, i.e., the sum of the number of boxes it takes to find the
first to the $n$-th Monopoly Card, is then $Var(X) = Var(X_1) + Var(X_2) + \cdots 
+ Var(X_n)$ since the individual random variables are independent of one another.
The number of boxes it takes to find the first coupon has no bearing on the 
number of boxes it will take to find the next. Then, 
\[
	\begin{split}
		\Var(X) &= \Var(X_1) + \Var(X_2) + \dots + \Var(X_n) \\
		&= \left( \frac{1-p_1}{p_1^2} \right) + \left( \frac{1-p_2}{p_2^2} \right) + \ddots \\
		&= \left( \frac{1-1}{1^2} \right) + \left( \frac{1-\frac{n-1}{n}}{\left( \frac{n-1}{n} \right)^2} \right) + \cdots \\
		&= \left( \frac{1-1}{1^2} \right) + \left( \frac{n^2}{(n-1)^2} - \frac{n}{n-1} \right) + \left( \frac{n^2}{(n-2)^2} - \frac{n}{n-2} \right) + \cdot \\
		&= \left( \frac{n^2}{n^2} + \frac{n^2}{(n-1)^2}  + \frac{n^2}{(n-2)^2} + \dots \right) 
		- \left( \frac{n}{n} + \frac{n}{n-1} + \frac{n}{n-2} + \cdots \right) \\
		&= n^2 \cdot \left( \frac{1}{1} + \frac{1}{(2)^2}  + \dots + \frac{1}{(n)^2} \right) 
		- n \cdot \left( \frac{1}{1} + \frac{1}{2} + \dots + \frac{1}{n}  \right) \\
		&= n^2 \sum_{i=1}^{n} \frac{1}{i^2} - n \sum_{i=1}^{n} \frac{1}{i} \\
		&= n^2 \sum_{i=1}^{n} \frac{1}{i^2} - \E[X]
	\end{split}
\]

\end{solution}

\Question {Diversify Your Hand}

\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n15.pdf}{Note 15},
\href{https://www.eecs70.org/assets/pdf/notes/n16.pdf}{Note 16}}
You are dealt 5 cards from a standard 52 card deck. Let $X$ be the number of 
distinct values in your hand. For instance, the hand (A, A, A, 2, 3) has 3 
distinct values.

\begin{Parts}

\Part Calculate $\E[X]$. (Hint: Consider indicator variables $X_i$ representing 
whether $i$ appears in the hand.)

\Part Calculate $\var(X)$. The answer expression will be quite involved; you do 
not need to simplify anything.

\end{Parts}

\begin{solution}
	
\begin{Parts}
		
\Part The expected value of $X$, where $X = X_1 + X_2 + \dots + X_13$, is 
\[
	\begin{split}
		\E(X) &= \sum_{i=1}^{13} \E(X_i) \\
		&= \sum_{i=1}^{13} \Pr[X_i = 1] \\
		&= \sum_{i=1}^{13} (1 - \frac{\binom{48}{5}}{\binom{52}{5}}) \\
		&= 13 (1 - \frac{\binom{48}{5}}{\binom{52}{5}})
	\end{split}
\]

\Part The variance of $X$ is then 
\[
	\begin{split}
		\var(X) &= \E[X^2] - (\E[X])^2 \\
		&= \E[X] - (\E[X])^2 \\
		&= 13 \left( 1 - \frac{\binom{48}{5}}{\binom{52}{5}} \right) - \left(13 \left(1 - \frac{\binom{48}{5}}{\binom{52}{5}}\right) \right)^2 
	\end{split}
\]
	
\end{Parts}
	
\end{solution}	

\Question{Double-Check Your Intuition Again}

\notelinks*{\href{https://www.eecs70.org/assets/pdf/notes/n16.pdf}{Note 16}}
\begin{Parts}
	\Part You roll a fair six-sided die and record the result $X$. You roll the 
	die again and record the result $Y$.  
	\begin{Parts}
		\item What is $\cov (X+Y, X-Y)$?
		\item Prove that $X+Y$ and $X-Y$ are not independent.
	\end{Parts}

\end{Parts}

For each of the problems below, if you think the answer is "yes" then provide a 
proof. If you think the answer is "no", then provide a counterexample.

\begin{Parts}[resume]
	
	\Part If $X$ is a random variable and $\var (X) = 0$, then must $X$ be a 
	constant?

	\Part If $X$ is a random variable and $c$ is a constant, then is $\var(cX) 
	= c \var(X)$?
	
	\Part If $A$ and $B$ are random variables with nonzero standard deviations 
	and $\text{Corr} (A, B) = 0$, then are $A$ and $B$ independent?

	\Part If $X$ and $Y$ are not necessarily independent random variables, but 
	$\text{Corr} (X, Y) = 0$, and $X$ and $Y$ have nonzero standard deviations,
	then is $\var (X+Y) = \var(X) + \var(Y)$?
	
	\Part If $X$ and $Y$ are random variables then is $\E[\max (X, Y) \min (X, 
	Y)] = \E[X Y]$?
	
	\Part If $X$ and $Y$ are independent random variables with nonzero standard 
	deviations, then is $$\text{Corr}(\max(X, Y), \min(X, Y)) = \text{Corr}(X, 
	Y)?$$
	
\end{Parts}

\begin{solution}
	
\begin{Parts}
	
\Part For part (i), the covariance is equal to zero, as shown below:
\[
	\begin{split}
		\cov(X+Y, X-Y) &= \cov(X, X) - \cov(X, Y) + \cov(Y, X) - \cov(Y, Y) \\
		&= \cov(X, X) - \cov(Y, Y) \\
		&= \var(X) - \var(Y) \\
		&= 0
	\end{split}
\]

For part (ii), if $X+Y$ and $X-Y$ were independent, $\var((X+Y)+(X-Y))$ would equal
$\var(X+Y) + \var(X-Y) = \var(X) + \var(Y) + \var(X) - \var(Y) = 2\var(X)$, since
the result of two separate dice rolls are independent from one another. However, 
the variance actually equals $\Var((X+Y)+(X-Y)) = \var(2X) = 4\var(X) \neq 2\var(X)$,
and therefore $X+Y$ and $X-Y$ are not independent.

\Part Yes, if $X$ is a random variable and $\var(X) = 0$, then $X$ is constant. 
If the variance is equal to zero, $\var(X) = \E[(X-\E(X))^2] = 0 \rightarrow 
X - \E[X] = 0 \rightarrow X = \E[X]$, and since $\E[X]$ is constant, so must
$X$. 

\Part No, consider a random variable $X$ and $c=2$, then 
\[
	\var(2X) = \E[4X^2] - (\E[2X])^2 = 4 \E[X^2] - 4 (\E[X])^2 = 4\var(X) \neq 2\var(X)
\]

\Part No, not necessarily, if $\text{Corr}(X, Y) = 0$ and the variables have
non-zero standard deviations, then $\cov(X,Y) = 0$, which does not necessarily
imply that the variables are independent. For example, consider the variables
$X+Y$ and $X-Y$ from part (a).

\Part Yes, if $\text{Corr}(X, Y) = 0$ and the variables have non-zero standard 
deviations, then $\cov(X,Y) = 0$. In the general case for calculating the 
variance of two random variables $\var(X, Y) = \var(X) + \var(Y) - 2\cov(X, Y)$,
but since $\cov(X, Y) = 0$, then it reduces to $\var(X, Y) = \var(X) + \var(Y)$.

\Part Yes, because in all cases $X > Y$, $X < Y$, and $X = Y$, both sides of the
equality are equal. If $X=Y$, then $\E[XY] = \E[XX]$ and similarly both the $\min$
and $\max$ functions both return the same value which then also becomes $\E[XX]$. 
If the two random variables are not equal, then necessarily the minimum and maximum
functions either return $X, Y$ or $Y, X$, respectively to multiply $XY$ in the $\E$ 
function. 

\Part This question essentially asks if $\cov(\max(X, Y), \min(X, Y)) = \cov(X, Y)$,
since the denominators on both sides of the equality from the correlation 
formulas are the same and have non-zero standard deviations. The answer is yes, 
since from the previous part we have shown that $\E[\max (X, Y) \min (X, Y)] = 
\E[XY]$, and also $\E[\max(X,Y)]\E[\min(X,Y)] = \E[X]\E[Y]$, with the same logic
as before. If the random variables are equal, then the equality will be true since
the minimum and maximum functions output the same value. If they are not the same
then the minimum function will return one value, and the maximum necessarily
returns the other. 

\end{Parts}
	
\end{solution}

\Question{Dice Games}

\notelinks*{\href{https://www.eecs70.org/assets/pdf/notes/n20.pdf}{Note 20}}
\begin{Parts}

\Part Alice rolls a die until she gets a 1. Let $X$ be the number of total rolls 
she makes (including the last one), and let $Y$ be the number of rolls on which 
she gets an even number. Compute $\E[Y \mid X = x]$, and use it to calculate 
$\E[Y]$. 

\Part Bob plays a game in which he starts off with one die. At each time step, 
he rolls all the dice he has. Then, for each die, if it comes up as an odd 
number, he puts that die back, and adds a number of dice equal to the number 
displayed to his collection. (For example, if he rolls a one on the first time 
step, he puts that die back along with an extra die.) However, if it comes up 
as an even number, he removes that die from his collection.

Compute the expected number of dice Bob will have after $n$ time steps. (Hint: 
compute the value of $\E[X_k \mid X_{k-1} = m]$ to derive a recursive expression 
for $X_k$, where $X_i$ is the random variable representing the number of dice 
after $i$ time steps. )

\end{Parts}

\begin{solution}
	
\begin{Parts}
	
\Part The expected value of the random variable $Y$ is 3. The first variable is
a geometrically distributed random variable with a probability of success of 
$\frac{1}{6}$. The second variable is a binomially distributed random variable
where the number of trials is the value that $X$ takes on, minus the last one, 
with a probability of success of $\frac{3}{5}$ (there are only 5 numbers to 
consider since it can't be 1). The expected value of a binomially distributed
variable like $Y$ is $n \times p$ (consider a series of indicator random 
variables), therefore, $\E[Y \mid X = x] = (x-1) \times \frac{3}{5}$. To find
the expected value of $Y$ in general, the expected value of $Y \mid X = x$ gives
the answer: $$\E[Y] = \E[\E[Y \mid X =x ]] = \E[(X-1) \times \frac{3}{5}] = (\E[X]
- 1) \times \frac{3}{5} = (6 -1 ) \times \frac{3}{5} = 3$$.

\Part I instead consider a random variable ``$+X_i$'', which represents the 
added number of dice from one particular die. $+X_i$, which I will now refer to
as $X_i$, has the following distribution: 
\[
X_i =
\begin{cases}
  -1, & \text{w.p. } \frac{1}{2} \\
  +1, & \text{w.p. } \frac{1}{6} \\
  +3, & \text{w.p. } \frac{1}{6} \\
  +5, & \text{w.p. } \frac{1}{6}.
\end{cases}
\]

The expected value for any $X_i$ is then $$\E[X_i] = \frac{1}{2}(- 1) + 
\frac{1}{6}(1+3+5)= - \frac{1}{2} + \frac{3}{2} = 1.$$

For a given $k$ then, $$\E[+X_k \mid X_{k-1} = m] = \E[X_1 + X_2 + \dots + X_m] =
\E[X_1] + \E[X_2] + \dots + \E[X_m] = m.$$ Therefore, the
expected value of $X_k$ (not $+X_k$) is the number of dice already in existence,
$m$, plus the additional dice, $m$, so $\E[X_k \mid X_{k-1} = m] = m + m = 2m$. 
The expected value of $X_n$ follows a recursive expansion, $\E[X_n] = 2 \E[X_{n-1}]
=  2^2 \E[X_{n-2}] = \dots = 2^n \E[X_0] = 2^n \times 1 = 2^n$.

\end{Parts}

\end{solution}

\Question{LLSE and Graphs}
\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n20.pdf}{Note 20}}
Consider a graph with $n$ vertices numbered $1$ through $n$, where $n$ is a 
positive integer $\ge 2$. For each pair of distinct vertices, we add an 
undirected edge between them independently with probability $p$. Let $D_1$ be 
the random variable representing the degree of vertex 1, and let $D_2$ be the 
random variable representing the degree of vertex 2. 

\begin{Parts}
	\Part Compute $\E[D_1]$ and $\E[D_2]$.
	\Part Compute $\var(D_1)$. 
	\Part Compute $\cov(D_1, D_2)$.
    \Part Using the information from the first three parts, what is $L(D_2 \mid D_1)$?
\end{Parts}

\begin{solution}
	
\begin{Parts}
	
\Part $D_1$ can be represented as a series of $n-1$ indicator random variables, 
each representing the existence of an edge between the vertex and the other 
vertices. So, $D_1 = X_2 + X_3 + \dots + X_n$. Then, the expected value is 
$\E[D_1] = \E[X_2 + X_3 + \dots + X_n] = \E[X_2] + \E[X_3] + \dots + \E[X_n] = 
(n-1)p$. $\E[D_2]$ takes on the same value. 

\Part The variance of $D_1$ is $\var(X_2 + X_3 + \dots + X_n) = 
\var(X_2) + \var(X_3) + \dots + \var(X_n) = (n-1)(p-p^2)$.

\Part The covariance of $D_1$ and $D_2$ boils down to the variance of $X_2$, as
seen below:
\[
	\begin{split}
		\cov(D_1, D_2) &= \cov(X_2 + X_3 + \dots + X_n, Y_1 + Y_3 + \dots + Y_n) \\
		&= \cov(X_2 + X_3 + \dots + X_n, Y_1 + Y_3 + \dots + Y_n) \\
		&= \sum_{i \neq 1}^{n} \sum_{j \neq 2}^{n} \cov(X_i, Y_i) \\
		&= \cov(X_2, Y_1) \quad \text{Variables otherwise independent} \\
		&= \cov(X_2, X_2) \quad \text{$X_2$ and $Y_1$ represent same state} \\ 
		&= \var(X_2) \\
		&= p - p^2 
	\end{split}
\]

\Part The linear least squares estimate of $D_2$ given $D_1$ is 
\[
	\begin{split}
		\hat{D}_2 &= \E[D_2] + \frac{\cov(D_1, D_2)}{\var(D_1)}(D_1 - \E[D_1]) \\
		&= (n-1)p + \frac{p-p^2}{(n-1)(p-p^2)}(D_1 - (n-1)p) \\
		&= (n-1)p + \frac{D_1}{n-1} - p \\
	\end{split}
\]

\end{Parts}

\end{solution}

\Question{Balls in Bins Estimation}
\notelinks{\href{https://www.eecs70.org/assets/pdf/notes/n20.pdf}{Note 20}}
We throw $n > 0$ balls into $m \geq 2$ bins. Let $X$ and $Y$ represent the number of balls that land in bin $1$ and $2$ respectively.

\begin{Parts}

    \Part Calculate $\E[Y \mid X]$. [\textit{Hint}: Your intuition may be more useful than formal calculations.]

    \Part What is $L[Y \mid X]$ (where $L[Y \mid X]$ is the best linear estimator of $Y$ given $X$)? [\textit{Hint}: Your justification should be no more than two or three sentences, no calculations necessary! Think carefully about the meaning of the conditional expectation.]

  \Part Unfortunately, your friend is not convinced by your answer to the previous part. Compute $\E[X]$ and $\E[Y]$.

  \Part Compute $\var(X)$.

  \Part Compute $\cov(X, Y)$.

  \Part Compute $L[Y \mid X]$ using the formula. Ensure that your answer is the same as your answer to part (b).

\end{Parts}

\end{document}
